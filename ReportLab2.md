## Алгоритмы

- Логистическая регрессия. Метод линейной модели с градиентным спуском.
- Метод ближайших соседей. Используется Евклидова норма. Наивная реализация.
- Решающее дерево. Я решил взять коэффициент Джини при разбиении, а не энтропийный критерий. 
- Случайный лес. При создании деревьев производится бустрапирование выборки. Согласно оригинальной статье и указаниям в блоге Дъяконова для каждого дерева случанйым образом выбирается подвыборка признаков в размере sqrt(n) от исходного количества (только для задачи классификации).

##  Результаты

Данные алгоритмы были применены к обоим наборам данных и показали сравнимый результат по отношению к аналогичным моделям из библиотеки Scikit-learn по метрике accuracy.

Достаточно странно, что если моя логистическая регрессия показывала сравнимую производительность с оптимизированной библиотекой, то ничуть не более сложный метод ближайших соседей на одной задаче показал почти одинаковую производительность в моей и готовой реализации, а на другом датасете моя версия отработала на порядок дольше.

Что касается "деревянных" моделей, то мое решающее дерево показало лучший результат в сравнении с sklearn, однако худшую производительность. Это связано с тем, что моя реализация не предусматривает никаких оптимзиаций и строит дерево самым обычным образом. Данная проблема, разумеется, встречается и в моем случайном лесе, использующем те же самые деревья, но также стоит отметить, что моя реализация не предусматривает параллельное построение деревьев, поэтому время работы алгоритма оставляет желать лучшего.